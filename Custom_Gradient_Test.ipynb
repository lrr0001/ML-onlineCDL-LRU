{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This Python Notebook demonstrates custom gradients and a custom loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, the operation with custom gradient is an identity function, and it's custom gradient will flip the sign of the gradient. As a result, models that include this custom gradient will maximize the loss instead of minimizing it. So that this is visually clear, I will use a strange custom loss function, plotted below. The loss is minimized when the (ground truth) \"actual output\" is 5 less than the output predicted by the model and maximized when the \"actual output\" is 5 greater than the output predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def custom_loss(y_actual, y_pred):\n",
    "    pred_rank = tf.rank(y_pred)\n",
    "    if pred_rank > 1:\n",
    "        return tf.math.reduce_sum(1/((y_actual - y_pred - 5)**2 + 3) - 1/((y_actual - y_pred + 5)**2 + 3),axis=tf.range(1,tf.rank(y_pred)))\n",
    "    else:\n",
    "        return 1/((y_actual - y_pred - 5)**2 + 3) - 1/((y_actual - y_pred + 5)**2 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f10ed978b70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFdCAYAAAAqi+WzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10ee45ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_actual = np.arange(-10,10,0.05)\n",
    "y_predicted = np.zeros(y_actual.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "axs = fig.add_axes([0,0,1,1])\n",
    "#loss = lambda act,pred: np.array([custom_loss(act[ii],pred[ii]) for ii in range(act.shape[0])])\n",
    "\n",
    "axs.plot(y_actual - y_predicted,custom_loss(y_actual,y_predicted))\n",
    "axs.set_title('Custom Loss Function')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.set_xlabel('Actual Minus Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first example does not include custom gradients. The custom loss has a minimum just below -0.3 and a maximum just above 0.3. It reaches its minimum when the model predicts outputs 5 larger than the \"expected output.\"\n",
    "\n",
    "The model learns as expected, successfully minimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/96\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2268 - val_loss: 0.2163\n",
      "Epoch 2/96\n",
      "100/100 [==============================] - 0s 806us/step - loss: 0.2166 - val_loss: 0.2059\n",
      "Epoch 3/96\n",
      "100/100 [==============================] - 0s 884us/step - loss: 0.2070 - val_loss: 0.1960\n",
      "Epoch 4/96\n",
      "100/100 [==============================] - 0s 810us/step - loss: 0.1978 - val_loss: 0.1865\n",
      "Epoch 5/96\n",
      "100/100 [==============================] - 0s 852us/step - loss: 0.1891 - val_loss: 0.1775\n",
      "Epoch 6/96\n",
      "100/100 [==============================] - 0s 926us/step - loss: 0.1807 - val_loss: 0.1689\n",
      "Epoch 7/96\n",
      "100/100 [==============================] - 0s 898us/step - loss: 0.1725 - val_loss: 0.1602\n",
      "Epoch 8/96\n",
      "100/100 [==============================] - 0s 863us/step - loss: 0.1643 - val_loss: 0.1516\n",
      "Epoch 9/96\n",
      "100/100 [==============================] - 0s 868us/step - loss: 0.1559 - val_loss: 0.1430\n",
      "Epoch 10/96\n",
      "100/100 [==============================] - 0s 915us/step - loss: 0.1473 - val_loss: 0.1342\n",
      "Epoch 11/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.1380 - val_loss: 0.1252\n",
      "Epoch 12/96\n",
      "100/100 [==============================] - 0s 844us/step - loss: 0.1288 - val_loss: 0.1166\n",
      "Epoch 13/96\n",
      "100/100 [==============================] - 0s 921us/step - loss: 0.1193 - val_loss: 0.1082\n",
      "Epoch 14/96\n",
      "100/100 [==============================] - 0s 862us/step - loss: 0.1103 - val_loss: 0.1005\n",
      "Epoch 15/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.1019 - val_loss: 0.0933\n",
      "Epoch 16/96\n",
      "100/100 [==============================] - 0s 883us/step - loss: 0.0939 - val_loss: 0.0865\n",
      "Epoch 17/96\n",
      "100/100 [==============================] - 0s 922us/step - loss: 0.0868 - val_loss: 0.0805\n",
      "Epoch 18/96\n",
      "100/100 [==============================] - 0s 877us/step - loss: 0.0802 - val_loss: 0.0748\n",
      "Epoch 19/96\n",
      "100/100 [==============================] - 0s 964us/step - loss: 0.0742 - val_loss: 0.0695\n",
      "Epoch 20/96\n",
      "100/100 [==============================] - 0s 914us/step - loss: 0.0686 - val_loss: 0.0646\n",
      "Epoch 21/96\n",
      "100/100 [==============================] - 0s 883us/step - loss: 0.0635 - val_loss: 0.0600\n",
      "Epoch 22/96\n",
      "100/100 [==============================] - 0s 925us/step - loss: 0.0588 - val_loss: 0.0556\n",
      "Epoch 23/96\n",
      "100/100 [==============================] - 0s 865us/step - loss: 0.0544 - val_loss: 0.0515\n",
      "Epoch 24/96\n",
      "100/100 [==============================] - 0s 906us/step - loss: 0.0502 - val_loss: 0.0476\n",
      "Epoch 25/96\n",
      "100/100 [==============================] - 0s 913us/step - loss: 0.0463 - val_loss: 0.0439\n",
      "Epoch 26/96\n",
      "100/100 [==============================] - 0s 875us/step - loss: 0.0425 - val_loss: 0.0404\n",
      "Epoch 27/96\n",
      "100/100 [==============================] - 0s 901us/step - loss: 0.0390 - val_loss: 0.0370\n",
      "Epoch 28/96\n",
      "100/100 [==============================] - 0s 916us/step - loss: 0.0356 - val_loss: 0.0337\n",
      "Epoch 29/96\n",
      "100/100 [==============================] - 0s 955us/step - loss: 0.0323 - val_loss: 0.0306\n",
      "Epoch 30/96\n",
      "100/100 [==============================] - 0s 913us/step - loss: 0.0292 - val_loss: 0.0275\n",
      "Epoch 31/96\n",
      "100/100 [==============================] - 0s 834us/step - loss: 0.0262 - val_loss: 0.0246\n",
      "Epoch 32/96\n",
      "100/100 [==============================] - 0s 845us/step - loss: 0.0232 - val_loss: 0.0217\n",
      "Epoch 33/96\n",
      "100/100 [==============================] - 0s 845us/step - loss: 0.0204 - val_loss: 0.0189\n",
      "Epoch 34/96\n",
      "100/100 [==============================] - 0s 859us/step - loss: 0.0176 - val_loss: 0.0162\n",
      "Epoch 35/96\n",
      "100/100 [==============================] - 0s 913us/step - loss: 0.0149 - val_loss: 0.0135\n",
      "Epoch 36/96\n",
      "100/100 [==============================] - 0s 836us/step - loss: 0.0122 - val_loss: 0.0109\n",
      "Epoch 37/96\n",
      "100/100 [==============================] - 0s 955us/step - loss: 0.0096 - val_loss: 0.0083\n",
      "Epoch 38/96\n",
      "100/100 [==============================] - 0s 910us/step - loss: 0.0070 - val_loss: 0.0057\n",
      "Epoch 39/96\n",
      "100/100 [==============================] - 0s 920us/step - loss: 0.0044 - val_loss: 0.0032\n",
      "Epoch 40/96\n",
      "100/100 [==============================] - 0s 929us/step - loss: 0.0018 - val_loss: 6.4740e-04\n",
      "Epoch 41/96\n",
      "100/100 [==============================] - 0s 828us/step - loss: -7.1597e-04 - val_loss: -0.0019\n",
      "Epoch 42/96\n",
      "100/100 [==============================] - 0s 843us/step - loss: -0.0033 - val_loss: -0.0044\n",
      "Epoch 43/96\n",
      "100/100 [==============================] - 0s 881us/step - loss: -0.0058 - val_loss: -0.0070\n",
      "Epoch 44/96\n",
      "100/100 [==============================] - 0s 931us/step - loss: -0.0084 - val_loss: -0.0096\n",
      "Epoch 45/96\n",
      "100/100 [==============================] - 0s 854us/step - loss: -0.0111 - val_loss: -0.0122\n",
      "Epoch 46/96\n",
      "100/100 [==============================] - 0s 970us/step - loss: -0.0137 - val_loss: -0.0148\n",
      "Epoch 47/96\n",
      "100/100 [==============================] - 0s 906us/step - loss: -0.0165 - val_loss: -0.0176\n",
      "Epoch 48/96\n",
      "100/100 [==============================] - 0s 899us/step - loss: -0.0192 - val_loss: -0.0203\n",
      "Epoch 49/96\n",
      "100/100 [==============================] - 0s 818us/step - loss: -0.0221 - val_loss: -0.0232\n",
      "Epoch 50/96\n",
      "100/100 [==============================] - 0s 888us/step - loss: -0.0251 - val_loss: -0.0261\n",
      "Epoch 51/96\n",
      "100/100 [==============================] - 0s 838us/step - loss: -0.0281 - val_loss: -0.0291\n",
      "Epoch 52/96\n",
      "100/100 [==============================] - 0s 849us/step - loss: -0.0313 - val_loss: -0.0322\n",
      "Epoch 53/96\n",
      "100/100 [==============================] - 0s 878us/step - loss: -0.0346 - val_loss: -0.0355\n",
      "Epoch 54/96\n",
      "100/100 [==============================] - 0s 859us/step - loss: -0.0381 - val_loss: -0.0389\n",
      "Epoch 55/96\n",
      "100/100 [==============================] - 0s 838us/step - loss: -0.0418 - val_loss: -0.0425\n",
      "Epoch 56/96\n",
      "100/100 [==============================] - 0s 895us/step - loss: -0.0457 - val_loss: -0.0462\n",
      "Epoch 57/96\n",
      "100/100 [==============================] - 0s 830us/step - loss: -0.0499 - val_loss: -0.0502\n",
      "Epoch 58/96\n",
      "100/100 [==============================] - 0s 834us/step - loss: -0.0544 - val_loss: -0.0545\n",
      "Epoch 59/96\n",
      "100/100 [==============================] - 0s 908us/step - loss: -0.0593 - val_loss: -0.0591\n",
      "Epoch 60/96\n",
      "100/100 [==============================] - 0s 826us/step - loss: -0.0646 - val_loss: -0.0641\n",
      "Epoch 61/96\n",
      "100/100 [==============================] - 0s 825us/step - loss: -0.0705 - val_loss: -0.0695\n",
      "Epoch 62/96\n",
      "100/100 [==============================] - 0s 794us/step - loss: -0.0771 - val_loss: -0.0754\n",
      "Epoch 63/96\n",
      "100/100 [==============================] - 0s 891us/step - loss: -0.0842 - val_loss: -0.0819\n",
      "Epoch 64/96\n",
      "100/100 [==============================] - 0s 790us/step - loss: -0.0919 - val_loss: -0.0887\n",
      "Epoch 65/96\n",
      "100/100 [==============================] - 0s 833us/step - loss: -0.1001 - val_loss: -0.0962\n",
      "Epoch 66/96\n",
      "100/100 [==============================] - 0s 971us/step - loss: -0.1085 - val_loss: -0.1039\n",
      "Epoch 67/96\n",
      "100/100 [==============================] - 0s 901us/step - loss: -0.1170 - val_loss: -0.1118\n",
      "Epoch 68/96\n",
      "100/100 [==============================] - 0s 882us/step - loss: -0.1251 - val_loss: -0.1197\n",
      "Epoch 69/96\n",
      "100/100 [==============================] - 0s 856us/step - loss: -0.1330 - val_loss: -0.1275\n",
      "Epoch 70/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: -0.1406 - val_loss: -0.1352\n",
      "Epoch 71/96\n",
      "100/100 [==============================] - 0s 850us/step - loss: -0.1476 - val_loss: -0.1426\n",
      "Epoch 72/96\n",
      "100/100 [==============================] - 0s 852us/step - loss: -0.1547 - val_loss: -0.1502\n",
      "Epoch 73/96\n",
      "100/100 [==============================] - 0s 893us/step - loss: -0.1618 - val_loss: -0.1578\n",
      "Epoch 74/96\n",
      "100/100 [==============================] - 0s 907us/step - loss: -0.1690 - val_loss: -0.1655\n",
      "Epoch 75/96\n",
      "100/100 [==============================] - 0s 827us/step - loss: -0.1765 - val_loss: -0.1736\n",
      "Epoch 76/96\n",
      "100/100 [==============================] - 0s 892us/step - loss: -0.1844 - val_loss: -0.1821\n",
      "Epoch 77/96\n",
      "100/100 [==============================] - 0s 929us/step - loss: -0.1927 - val_loss: -0.1911\n",
      "Epoch 78/96\n",
      "100/100 [==============================] - 0s 821us/step - loss: -0.2014 - val_loss: -0.2004\n",
      "Epoch 79/96\n",
      "100/100 [==============================] - 0s 879us/step - loss: -0.2107 - val_loss: -0.2105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/96\n",
      "100/100 [==============================] - 0s 905us/step - loss: -0.2206 - val_loss: -0.2211\n",
      "Epoch 81/96\n",
      "100/100 [==============================] - 0s 880us/step - loss: -0.2310 - val_loss: -0.2323\n",
      "Epoch 82/96\n",
      "100/100 [==============================] - 0s 919us/step - loss: -0.2420 - val_loss: -0.2443\n",
      "Epoch 83/96\n",
      "100/100 [==============================] - 0s 865us/step - loss: -0.2535 - val_loss: -0.2565\n",
      "Epoch 84/96\n",
      "100/100 [==============================] - 0s 845us/step - loss: -0.2653 - val_loss: -0.2690\n",
      "Epoch 85/96\n",
      "100/100 [==============================] - 0s 942us/step - loss: -0.2774 - val_loss: -0.2815\n",
      "Epoch 86/96\n",
      "100/100 [==============================] - 0s 882us/step - loss: -0.2890 - val_loss: -0.2933\n",
      "Epoch 87/96\n",
      "100/100 [==============================] - 0s 783us/step - loss: -0.2999 - val_loss: -0.3042\n",
      "Epoch 88/96\n",
      "100/100 [==============================] - 0s 891us/step - loss: -0.3095 - val_loss: -0.3131\n",
      "Epoch 89/96\n",
      "100/100 [==============================] - 0s 960us/step - loss: -0.3167 - val_loss: -0.3194\n",
      "Epoch 90/96\n",
      "100/100 [==============================] - 0s 944us/step - loss: -0.3214 - val_loss: -0.3228\n",
      "Epoch 91/96\n",
      "100/100 [==============================] - 0s 839us/step - loss: -0.3234 - val_loss: -0.3236\n",
      "Epoch 92/96\n",
      "100/100 [==============================] - 0s 847us/step - loss: -0.3236 - val_loss: -0.3236\n",
      "Epoch 93/96\n",
      "100/100 [==============================] - 0s 906us/step - loss: -0.3236 - val_loss: -0.3236\n",
      "Epoch 94/96\n",
      "100/100 [==============================] - 0s 926us/step - loss: -0.3236 - val_loss: -0.3236\n",
      "Epoch 95/96\n",
      "100/100 [==============================] - 0s 896us/step - loss: -0.3236 - val_loss: -0.3236\n",
      "Epoch 96/96\n",
      "100/100 [==============================] - 0s 885us/step - loss: -0.3236 - val_loss: -0.3236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-2.0008945]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([9.007946], dtype=float32)>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "outputs = tf.keras.layers.Dense(units=1,use_bias=True)(inputs)\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "model.compile(loss = custom_loss)\n",
    "x = np.random.randn(1000,1)\n",
    "y = -2*x + 4\n",
    "xval = np.random.randn(100,1)\n",
    "yval = -2*xval + 4\n",
    "model.fit(x=x,y=y,batch_size=10,epochs=96,validation_data = (xval,yval))\n",
    "model.trainable_variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell includes a custom gradient.  The layer with a custom gradient does nothing to its input, but flips the sign of the gradient.\n",
    "\n",
    "The result: the loss is maximized instead of minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def flip_grad(x):\n",
    "    def grad(dy):\n",
    "        return -dy\n",
    "    return tf.identity(x), grad\n",
    "\n",
    "class FlipGrad(tf.keras.layers.Layer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "    def call(self,inputs):\n",
    "        return flip_grad(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/96\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1772 - val_loss: 0.1764\n",
      "Epoch 2/96\n",
      "100/100 [==============================] - 0s 855us/step - loss: 0.1835 - val_loss: 0.1832\n",
      "Epoch 3/96\n",
      "100/100 [==============================] - 0s 928us/step - loss: 0.1902 - val_loss: 0.1906\n",
      "Epoch 4/96\n",
      "100/100 [==============================] - 0s 915us/step - loss: 0.1973 - val_loss: 0.1984\n",
      "Epoch 5/96\n",
      "100/100 [==============================] - 0s 909us/step - loss: 0.2047 - val_loss: 0.2062\n",
      "Epoch 6/96\n",
      "100/100 [==============================] - 0s 858us/step - loss: 0.2123 - val_loss: 0.2147\n",
      "Epoch 7/96\n",
      "100/100 [==============================] - 0s 831us/step - loss: 0.2204 - val_loss: 0.2235\n",
      "Epoch 8/96\n",
      "100/100 [==============================] - 0s 829us/step - loss: 0.2290 - val_loss: 0.2330\n",
      "Epoch 9/96\n",
      "100/100 [==============================] - 0s 856us/step - loss: 0.2378 - val_loss: 0.2425\n",
      "Epoch 10/96\n",
      "100/100 [==============================] - 0s 834us/step - loss: 0.2470 - val_loss: 0.2526\n",
      "Epoch 11/96\n",
      "100/100 [==============================] - 0s 881us/step - loss: 0.2563 - val_loss: 0.2627\n",
      "Epoch 12/96\n",
      "100/100 [==============================] - 0s 841us/step - loss: 0.2659 - val_loss: 0.2726\n",
      "Epoch 13/96\n",
      "100/100 [==============================] - 0s 865us/step - loss: 0.2754 - val_loss: 0.2825\n",
      "Epoch 14/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.2843 - val_loss: 0.2914\n",
      "Epoch 15/96\n",
      "100/100 [==============================] - 0s 835us/step - loss: 0.2931 - val_loss: 0.2997\n",
      "Epoch 16/96\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.3010 - val_loss: 0.3070\n",
      "Epoch 17/96\n",
      "100/100 [==============================] - 0s 903us/step - loss: 0.3082 - val_loss: 0.3131\n",
      "Epoch 18/96\n",
      "100/100 [==============================] - 0s 902us/step - loss: 0.3143 - val_loss: 0.3179\n",
      "Epoch 19/96\n",
      "100/100 [==============================] - 0s 825us/step - loss: 0.3190 - val_loss: 0.3213\n",
      "Epoch 20/96\n",
      "100/100 [==============================] - 0s 880us/step - loss: 0.3221 - val_loss: 0.3232\n",
      "Epoch 21/96\n",
      "100/100 [==============================] - 0s 961us/step - loss: 0.3235 - val_loss: 0.3236\n",
      "Epoch 22/96\n",
      "100/100 [==============================] - 0s 903us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 23/96\n",
      "100/100 [==============================] - 0s 848us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 24/96\n",
      "100/100 [==============================] - 0s 867us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 25/96\n",
      "100/100 [==============================] - 0s 924us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 26/96\n",
      "100/100 [==============================] - 0s 931us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 27/96\n",
      "100/100 [==============================] - 0s 887us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 28/96\n",
      "100/100 [==============================] - 0s 924us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 29/96\n",
      "100/100 [==============================] - 0s 871us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 30/96\n",
      "100/100 [==============================] - 0s 904us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 31/96\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 32/96\n",
      "100/100 [==============================] - 0s 889us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 33/96\n",
      "100/100 [==============================] - 0s 922us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 34/96\n",
      "100/100 [==============================] - 0s 847us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 35/96\n",
      "100/100 [==============================] - 0s 955us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 36/96\n",
      "100/100 [==============================] - 0s 903us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 37/96\n",
      "100/100 [==============================] - 0s 864us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 38/96\n",
      "100/100 [==============================] - 0s 880us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 39/96\n",
      "100/100 [==============================] - 0s 917us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 40/96\n",
      "100/100 [==============================] - 0s 931us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 41/96\n",
      "100/100 [==============================] - 0s 904us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 42/96\n",
      "100/100 [==============================] - 0s 933us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 43/96\n",
      "100/100 [==============================] - 0s 849us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 44/96\n",
      "100/100 [==============================] - 0s 904us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 45/96\n",
      "100/100 [==============================] - 0s 895us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 46/96\n",
      "100/100 [==============================] - 0s 892us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 47/96\n",
      "100/100 [==============================] - 0s 867us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 48/96\n",
      "100/100 [==============================] - 0s 876us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 49/96\n",
      "100/100 [==============================] - 0s 910us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 50/96\n",
      "100/100 [==============================] - 0s 968us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 51/96\n",
      "100/100 [==============================] - 0s 956us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 52/96\n",
      "100/100 [==============================] - 0s 925us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 53/96\n",
      "100/100 [==============================] - 0s 818us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 54/96\n",
      "100/100 [==============================] - 0s 867us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 55/96\n",
      "100/100 [==============================] - 0s 911us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 56/96\n",
      "100/100 [==============================] - 0s 906us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 57/96\n",
      "100/100 [==============================] - 0s 882us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 58/96\n",
      "100/100 [==============================] - 0s 782us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 59/96\n",
      "100/100 [==============================] - 0s 897us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 60/96\n",
      "100/100 [==============================] - 0s 904us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 61/96\n",
      "100/100 [==============================] - 0s 873us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 62/96\n",
      "100/100 [==============================] - 0s 809us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 63/96\n",
      "100/100 [==============================] - 0s 855us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 64/96\n",
      "100/100 [==============================] - 0s 843us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 65/96\n",
      "100/100 [==============================] - 0s 829us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 66/96\n",
      "100/100 [==============================] - 0s 868us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 67/96\n",
      "100/100 [==============================] - 0s 811us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 68/96\n",
      "100/100 [==============================] - 0s 829us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 69/96\n",
      "100/100 [==============================] - 0s 895us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 70/96\n",
      "100/100 [==============================] - 0s 853us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 71/96\n",
      "100/100 [==============================] - 0s 798us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 72/96\n",
      "100/100 [==============================] - 0s 840us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 73/96\n",
      "100/100 [==============================] - 0s 848us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 74/96\n",
      "100/100 [==============================] - 0s 932us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 75/96\n",
      "100/100 [==============================] - 0s 880us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 76/96\n",
      "100/100 [==============================] - 0s 885us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 77/96\n",
      "100/100 [==============================] - 0s 872us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 78/96\n",
      "100/100 [==============================] - 0s 921us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 79/96\n",
      "100/100 [==============================] - 0s 957us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 80/96\n",
      "100/100 [==============================] - 0s 899us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 81/96\n",
      "100/100 [==============================] - 0s 855us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 82/96\n",
      "100/100 [==============================] - 0s 844us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 83/96\n",
      "100/100 [==============================] - 0s 885us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 84/96\n",
      "100/100 [==============================] - 0s 957us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 85/96\n",
      "100/100 [==============================] - 0s 879us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 86/96\n",
      "100/100 [==============================] - 0s 868us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 87/96\n",
      "100/100 [==============================] - 0s 914us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 88/96\n",
      "100/100 [==============================] - 0s 950us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 89/96\n",
      "100/100 [==============================] - 0s 840us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 90/96\n",
      "100/100 [==============================] - 0s 857us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 91/96\n",
      "100/100 [==============================] - 0s 850us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 92/96\n",
      "100/100 [==============================] - 0s 829us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 93/96\n",
      "100/100 [==============================] - 0s 888us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 94/96\n",
      "100/100 [==============================] - 0s 833us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 95/96\n",
      "100/100 [==============================] - 0s 894us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 96/96\n",
      "100/100 [==============================] - 0s 819us/step - loss: 0.3236 - val_loss: 0.3236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-2.0002277]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([-1.008253], dtype=float32)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "preOutput = tf.keras.layers.Dense(units=1,use_bias=True)(inputs)\n",
    "outputs = FlipGrad()(preOutput)\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "model.compile(loss = custom_loss)\n",
    "x = np.random.randn(1000,1)\n",
    "y = -2*x + 4\n",
    "xval = np.random.randn(100,1)\n",
    "yval = -2*xval + 4\n",
    "model.fit(x=x,y=y,batch_size=10,epochs=96,validation_data = (xval,yval))\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code cell defines the operation with the custom gradient inside the \\_\\_init\\_\\_ function of the custom layer. Like in the previous example, the sign of the gradient is flipped, and the model's fit function finds the weights and biases that maximize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlipGrad2(tf.keras.layers.Layer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        @tf.custom_gradient\n",
    "        def flip_grad2(x):\n",
    "            def grad(dy):\n",
    "                return -dy\n",
    "            return tf.identity(x), grad\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.f = lambda x: flip_grad2(x)\n",
    "    def call(self,inputs):\n",
    "        return self.f(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/96\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 0.1365\n",
      "Epoch 2/96\n",
      "100/100 [==============================] - 0s 924us/step - loss: 0.1526 - val_loss: 0.1418\n",
      "Epoch 3/96\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.1577 - val_loss: 0.1472\n",
      "Epoch 4/96\n",
      "100/100 [==============================] - 0s 947us/step - loss: 0.1628 - val_loss: 0.1526\n",
      "Epoch 5/96\n",
      "100/100 [==============================] - 0s 913us/step - loss: 0.1683 - val_loss: 0.1581\n",
      "Epoch 6/96\n",
      "100/100 [==============================] - 0s 936us/step - loss: 0.1737 - val_loss: 0.1638\n",
      "Epoch 7/96\n",
      "100/100 [==============================] - 0s 929us/step - loss: 0.1793 - val_loss: 0.1696\n",
      "Epoch 8/96\n",
      "100/100 [==============================] - 0s 975us/step - loss: 0.1851 - val_loss: 0.1756\n",
      "Epoch 9/96\n",
      "100/100 [==============================] - 0s 982us/step - loss: 0.1910 - val_loss: 0.1819\n",
      "Epoch 10/96\n",
      "100/100 [==============================] - 0s 830us/step - loss: 0.1971 - val_loss: 0.1883\n",
      "Epoch 11/96\n",
      "100/100 [==============================] - 0s 894us/step - loss: 0.2035 - val_loss: 0.1951\n",
      "Epoch 12/96\n",
      "100/100 [==============================] - 0s 880us/step - loss: 0.2100 - val_loss: 0.2022\n",
      "Epoch 13/96\n",
      "100/100 [==============================] - 0s 884us/step - loss: 0.2168 - val_loss: 0.2095\n",
      "Epoch 14/96\n",
      "100/100 [==============================] - 0s 912us/step - loss: 0.2238 - val_loss: 0.2172\n",
      "Epoch 15/96\n",
      "100/100 [==============================] - 0s 885us/step - loss: 0.2309 - val_loss: 0.2251\n",
      "Epoch 16/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.2383 - val_loss: 0.2332\n",
      "Epoch 17/96\n",
      "100/100 [==============================] - 0s 913us/step - loss: 0.2457 - val_loss: 0.2416\n",
      "Epoch 18/96\n",
      "100/100 [==============================] - 0s 923us/step - loss: 0.2535 - val_loss: 0.2501\n",
      "Epoch 19/96\n",
      "100/100 [==============================] - 0s 894us/step - loss: 0.2612 - val_loss: 0.2590\n",
      "Epoch 20/96\n",
      "100/100 [==============================] - 0s 851us/step - loss: 0.2690 - val_loss: 0.2677\n",
      "Epoch 21/96\n",
      "100/100 [==============================] - 0s 930us/step - loss: 0.2770 - val_loss: 0.2763\n",
      "Epoch 22/96\n",
      "100/100 [==============================] - 0s 845us/step - loss: 0.2847 - val_loss: 0.2849\n",
      "Epoch 23/96\n",
      "100/100 [==============================] - 0s 963us/step - loss: 0.2923 - val_loss: 0.2932\n",
      "Epoch 24/96\n",
      "100/100 [==============================] - 0s 860us/step - loss: 0.2996 - val_loss: 0.3011\n",
      "Epoch 25/96\n",
      "100/100 [==============================] - 0s 933us/step - loss: 0.3063 - val_loss: 0.3082\n",
      "Epoch 26/96\n",
      "100/100 [==============================] - 0s 886us/step - loss: 0.3124 - val_loss: 0.3142\n",
      "Epoch 27/96\n",
      "100/100 [==============================] - 0s 927us/step - loss: 0.3173 - val_loss: 0.3189\n",
      "Epoch 28/96\n",
      "100/100 [==============================] - 0s 849us/step - loss: 0.3208 - val_loss: 0.3220\n",
      "Epoch 29/96\n",
      "100/100 [==============================] - 0s 892us/step - loss: 0.3229 - val_loss: 0.3235\n",
      "Epoch 30/96\n",
      "100/100 [==============================] - 0s 984us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 31/96\n",
      "100/100 [==============================] - 0s 816us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 32/96\n",
      "100/100 [==============================] - 0s 880us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 33/96\n",
      "100/100 [==============================] - 0s 925us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 34/96\n",
      "100/100 [==============================] - 0s 897us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 35/96\n",
      "100/100 [==============================] - 0s 867us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 36/96\n",
      "100/100 [==============================] - 0s 874us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 37/96\n",
      "100/100 [==============================] - 0s 900us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 38/96\n",
      "100/100 [==============================] - 0s 889us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 39/96\n",
      "100/100 [==============================] - 0s 813us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 40/96\n",
      "100/100 [==============================] - 0s 914us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 41/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 42/96\n",
      "100/100 [==============================] - 0s 873us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 43/96\n",
      "100/100 [==============================] - 0s 960us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 44/96\n",
      "100/100 [==============================] - 0s 862us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 45/96\n",
      "100/100 [==============================] - 0s 883us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 46/96\n",
      "100/100 [==============================] - 0s 888us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 47/96\n",
      "100/100 [==============================] - 0s 827us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 48/96\n",
      "100/100 [==============================] - 0s 840us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 49/96\n",
      "100/100 [==============================] - 0s 858us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 50/96\n",
      "100/100 [==============================] - 0s 840us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 51/96\n",
      "100/100 [==============================] - 0s 832us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 52/96\n",
      "100/100 [==============================] - 0s 968us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 53/96\n",
      "100/100 [==============================] - 0s 863us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 54/96\n",
      "100/100 [==============================] - 0s 894us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 55/96\n",
      "100/100 [==============================] - 0s 840us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 56/96\n",
      "100/100 [==============================] - 0s 908us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 57/96\n",
      "100/100 [==============================] - 0s 880us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 58/96\n",
      "100/100 [==============================] - 0s 885us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 59/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 60/96\n",
      "100/100 [==============================] - 0s 877us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 61/96\n",
      "100/100 [==============================] - 0s 904us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 62/96\n",
      "100/100 [==============================] - 0s 922us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 63/96\n",
      "100/100 [==============================] - 0s 833us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 64/96\n",
      "100/100 [==============================] - 0s 868us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 65/96\n",
      "100/100 [==============================] - 0s 927us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 66/96\n",
      "100/100 [==============================] - 0s 861us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 67/96\n",
      "100/100 [==============================] - 0s 931us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 68/96\n",
      "100/100 [==============================] - 0s 923us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 69/96\n",
      "100/100 [==============================] - 0s 881us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 70/96\n",
      "100/100 [==============================] - 0s 860us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 71/96\n",
      "100/100 [==============================] - 0s 873us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 72/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 73/96\n",
      "100/100 [==============================] - 0s 857us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 74/96\n",
      "100/100 [==============================] - 0s 908us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 75/96\n",
      "100/100 [==============================] - 0s 941us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 76/96\n",
      "100/100 [==============================] - 0s 902us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 77/96\n",
      "100/100 [==============================] - 0s 922us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 78/96\n",
      "100/100 [==============================] - 0s 963us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 79/96\n",
      "100/100 [==============================] - 0s 869us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 80/96\n",
      "100/100 [==============================] - 0s 828us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 81/96\n",
      "100/100 [==============================] - 0s 901us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 82/96\n",
      "100/100 [==============================] - 0s 854us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 83/96\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 84/96\n",
      "100/100 [==============================] - 0s 889us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 85/96\n",
      "100/100 [==============================] - 0s 969us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 86/96\n",
      "100/100 [==============================] - 0s 970us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 87/96\n",
      "100/100 [==============================] - 0s 993us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 88/96\n",
      "100/100 [==============================] - 0s 911us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 89/96\n",
      "100/100 [==============================] - 0s 904us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 90/96\n",
      "100/100 [==============================] - 0s 921us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 91/96\n",
      "100/100 [==============================] - 0s 862us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 92/96\n",
      "100/100 [==============================] - 0s 920us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 93/96\n",
      "100/100 [==============================] - 0s 936us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 94/96\n",
      "100/100 [==============================] - 0s 901us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 95/96\n",
      "100/100 [==============================] - 0s 955us/step - loss: 0.3236 - val_loss: 0.3236\n",
      "Epoch 96/96\n",
      "100/100 [==============================] - 0s 877us/step - loss: 0.3236 - val_loss: 0.3236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-1.9998742]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([-1.0075637], dtype=float32)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "preOutput = tf.keras.layers.Dense(units=1,use_bias=True)(inputs)\n",
    "outputs = FlipGrad2()(preOutput)\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "model.compile(loss = custom_loss)\n",
    "x = np.random.randn(1000,1)\n",
    "y = -2*x + 4\n",
    "xval = np.random.randn(100,1)\n",
    "yval = -2*xval + 4\n",
    "model.fit(x=x,y=y,batch_size=10,epochs=96,validation_data = (xval,yval))\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
