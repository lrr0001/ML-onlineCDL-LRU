{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This compares the custom gradient of the Woodbury form to that of an equivalent layer with no decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 65.3752 - val_loss: 64.0160\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 64.7822 - val_loss: 63.4460\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 64.1837 - val_loss: 62.8708\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 63.5802 - val_loss: 62.2910\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 62.9723 - val_loss: 61.7073\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 62.3607 - val_loss: 61.1203\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 61.7461 - val_loss: 60.5306\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 61.1292 - val_loss: 59.9391\n",
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 65.3745 - val_loss: 64.0145\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 64.7798 - val_loss: 63.4429\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 64.1794 - val_loss: 62.8659\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 63.5740 - val_loss: 62.2843\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 62.9639 - val_loss: 61.6985\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 62.3500 - val_loss: 61.1093\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 61.7329 - val_loss: 60.5173\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 61.1133 - val_loss: 59.9231\n",
      "tf.Tensor(0.3532276430361058, shape=(), dtype=float64)\n",
      "tf.Tensor(7.805351707447112e-06, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matrix_decompositions_tf as fctr\n",
    "import transforms as transf\n",
    "import math\n",
    "import post_process_grad as ppg\n",
    "\n",
    "fltrSz = (3,3)\n",
    "fftSz = (8,8)\n",
    "noc = 3\n",
    "nof = 8\n",
    "rho = 1.0\n",
    "dtype = tf.complex128\n",
    "nots = 100\n",
    "novs = 100\n",
    "LRAparam = {'n_components': 3}\n",
    "\n",
    "AInv = fctr.dictionary_object2D_full(fltrSz=fltrSz,fftSz=fftSz,noc = noc,nof = nof,rho = rho,lraParam=LRAparam,dtype=dtype)\n",
    "\n",
    "wt = tf.complex(tf.random.normal((nots,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((nots,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wt = wt + tf.math.conj(tf.reverse(wt,axis=(2,3)))\n",
    "wt = wt[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "wv = tf.complex(tf.random.normal((novs,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((novs,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wv = wv + tf.math.conj(tf.reverse(wv,axis=(2,3)))\n",
    "wv = wv[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "\n",
    "B = transf.fft2d_inner(fftSz)(tf.random.normal((1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype))\n",
    "\n",
    "\n",
    "xt = tf.linalg.matmul(B,wt,adjoint_a = True)\n",
    "xv = tf.linalg.matmul(B,wv,adjoint_a = True)\n",
    "idmat = tf.eye(nof,dtype=dtype,batch_shape = (int(tf.rank(B)) - 2)*(1,))\n",
    "yt = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xt)\n",
    "yv = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xv)\n",
    "\n",
    "w = tf.keras.layers.Input(shape=(fftSz[0],math.floor(fftSz[1]/2.) + 1,) + (noc,1,),dtype=dtype)\n",
    "x = tf.linalg.matmul(B,w,adjoint_a = True)\n",
    "y1 = AInv.qinv(x)\n",
    "y2 = fctr.QInv_auto(AInv.dhmul,rho)(x)\n",
    "\n",
    "model1 = ppg.Model_record_grad(w,y2)\n",
    "model1.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model1.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "model2 = ppg.Model_passenger(model1.gradient_record,w,y1)\n",
    "model2.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model2.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "maxError = 0.\n",
    "maxGrad = 0.\n",
    "for grad1,grad2 in zip(model1.gradient_record,model2.gradient_record):\n",
    "    for gradval1,gradval2 in zip(grad1,grad2):\n",
    "        maxError = max(maxError,tf.math.reduce_max(tf.math.abs((gradval1 - gradval2)*tf.math.conj(gradval1 - gradval2))))\n",
    "        maxGrad = max(maxGrad,tf.math.reduce_max(tf.math.abs(gradval1*tf.math.conj(gradval1))),tf.math.reduce_max(tf.math.abs(gradval2*tf.math.conj(gradval2))))\n",
    "\n",
    "print(maxGrad)\n",
    "print(maxError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell compares the custom gradient for the standard form to that of an equivalent layer with automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 4.0546 - val_loss: 4.3840\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 4.0526 - val_loss: 4.3823\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 4.0507 - val_loss: 4.3806\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 4.0488 - val_loss: 4.3790\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 4.0470 - val_loss: 4.3774\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 4.0453 - val_loss: 4.3759\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 4.0436 - val_loss: 4.3744\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 4.0420 - val_loss: 4.3730\n",
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.0555 - val_loss: 4.3858\n",
      "tf.Tensor(0.01333136446801065, shape=(), dtype=float64)\n",
      "tf.Tensor(0.00031794270016761864, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matrix_decompositions_tf as fctr\n",
    "import transforms as transf\n",
    "import math\n",
    "import post_process_grad as ppg\n",
    "\n",
    "fltrSz = (3,3)\n",
    "fftSz = (8,8)\n",
    "noc = 8\n",
    "nof = 3\n",
    "rho = 1.0\n",
    "dtype = tf.complex128\n",
    "nots = 100\n",
    "novs = 100\n",
    "lraParam= {'n_components': 3}\n",
    "\n",
    "AInv = fctr.dictionary_object2D_full(fltrSz=fltrSz,fftSz=fftSz,noc = noc,nof = nof,rho = rho,lraParam=lraParam,dtype=dtype)\n",
    "\n",
    "wt = tf.complex(tf.random.normal((nots,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((nots,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wt = wt + tf.math.conj(tf.reverse(wt,axis=(2,3)))\n",
    "wt = wt[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "wv = tf.complex(tf.random.normal((novs,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((novs,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wv = wv + tf.math.conj(tf.reverse(wv,axis=(2,3)))\n",
    "wv = wv[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "\n",
    "B = transf.fft2d_inner(fftSz)(tf.random.normal((1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype))\n",
    "\n",
    "\n",
    "xt = tf.linalg.matmul(B,wt,adjoint_a = True)\n",
    "xv = tf.linalg.matmul(B,wv,adjoint_a = True)\n",
    "idmat = tf.eye(nof,dtype=dtype,batch_shape = (int(tf.rank(B)) - 2)*(1,))\n",
    "yt = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xt)\n",
    "yv = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xv)\n",
    "\n",
    "w = tf.keras.layers.Input(shape=(fftSz[0],math.floor(fftSz[1]/2.) + 1,) + (noc,1,),dtype=dtype)\n",
    "x = tf.linalg.matmul(B,w,adjoint_a = True)\n",
    "y1 = AInv.qinv(x)\n",
    "y2 = fctr.QInv_auto(AInv.dhmul,rho)(x)\n",
    "\n",
    "model1 = ppg.Model_record_grad(w,y2)\n",
    "model1.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model1.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "model2 = ppg.Model_passenger(model1.gradient_record,w,y1)\n",
    "model2.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model2.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "maxError = 0.\n",
    "maxGrad = 0.\n",
    "for grad1,grad2 in zip(model1.gradient_record,model2.gradient_record):\n",
    "    for gradval1,gradval2 in zip(grad1,grad2):\n",
    "        maxError = max(maxError,tf.math.reduce_max(tf.math.abs((gradval1 - gradval2)*tf.math.conj(gradval1 - gradval2))))\n",
    "        maxGrad = max(maxGrad,tf.math.reduce_max(tf.math.abs(gradval1*tf.math.conj(gradval1))),tf.math.reduce_max(tf.math.abs(gradval2*tf.math.conj(gradval2))))\n",
    "\n",
    "print(maxGrad)\n",
    "print(maxError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
