{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This compares the custom gradient of the Woodbury form to that of an equivalent layer with no decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 64.7896 - val_loss: 63.8838\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 64.0875 - val_loss: 63.2137\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 63.3788 - val_loss: 62.5375\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 62.6641 - val_loss: 61.8558\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 61.9439 - val_loss: 61.1693\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 61.2193 - val_loss: 60.4785\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 60.4910 - val_loss: 59.7844\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 59.7599 - val_loss: 59.0877\n",
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 64.7881 - val_loss: 63.8804\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 64.0824 - val_loss: 63.2067\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 63.3698 - val_loss: 62.5267\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 62.6508 - val_loss: 61.8409\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 61.9262 - val_loss: 61.1499\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 61.1967 - val_loss: 60.4545\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 60.4632 - val_loss: 59.7553\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 59.7265 - val_loss: 59.0533\n",
      "tf.Tensor(0.38005598276523966, shape=(), dtype=float64)\n",
      "tf.Tensor(0.00019220900037941968, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matrix_decompositions_tf as fctr\n",
    "import transforms as transf\n",
    "import math\n",
    "import post_process_grad as ppg\n",
    "\n",
    "fltrSz = (3,3)\n",
    "fftSz = (8,8)\n",
    "noc = 3\n",
    "nof = 8\n",
    "rho = 1.0\n",
    "dtype = tf.complex128\n",
    "nots = 100\n",
    "novs = 100\n",
    "\n",
    "AInv = fctr.dictionary_object2D_full(fltrSz=fltrSz,fftSz=fftSz,noc = noc,nof = nof,rho = rho,dtype=dtype)\n",
    "\n",
    "wt = tf.complex(tf.random.normal((nots,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((nots,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wt = wt + tf.math.conj(tf.reverse(wt,axis=(2,3)))\n",
    "wt = wt[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "wv = tf.complex(tf.random.normal((novs,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((novs,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wv = wv + tf.math.conj(tf.reverse(wv,axis=(2,3)))\n",
    "wv = wv[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "\n",
    "B = transf.fft2d_inner(fftSz)(tf.random.normal((1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype))\n",
    "\n",
    "\n",
    "xt = tf.linalg.matmul(B,wt,adjoint_a = True)\n",
    "xv = tf.linalg.matmul(B,wv,adjoint_a = True)\n",
    "idmat = tf.eye(nof,dtype=dtype,batch_shape = (int(tf.rank(B)) - 2)*(1,))\n",
    "yt = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xt)\n",
    "yv = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xv)\n",
    "\n",
    "w = tf.keras.layers.Input(shape=(fftSz[0],math.floor(fftSz[1]/2.) + 1,) + (noc,1,),dtype=dtype)\n",
    "x = tf.linalg.matmul(B,w,adjoint_a = True)\n",
    "y1 = AInv.qinv(x)\n",
    "y2 = fctr.QInv_auto(AInv.dhmul,rho)(x)\n",
    "\n",
    "model1 = ppg.Model_record_grad(w,y2)\n",
    "model1.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model1.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "model2 = ppg.Model_passenger(model1.gradient_record,w,y1)\n",
    "model2.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model2.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "maxError = 0.\n",
    "maxGrad = 0.\n",
    "for grad1,grad2 in zip(model1.gradient_record,model2.gradient_record):\n",
    "    for gradval1,gradval2 in zip(grad1,grad2):\n",
    "        maxError = max(maxError,tf.math.reduce_max(tf.math.abs((gradval1 - gradval2)*tf.math.conj(gradval1 - gradval2))))\n",
    "        maxGrad = max(maxGrad,tf.math.reduce_max(tf.math.abs(gradval1*tf.math.conj(gradval1))),tf.math.reduce_max(tf.math.abs(gradval2*tf.math.conj(gradval2))))\n",
    "\n",
    "print(maxGrad)\n",
    "print(maxError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell compares the custom gradient for the standard form to that of an equivalent layer with automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 5.2275 - val_loss: 5.7447\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 5.2273 - val_loss: 5.7446\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 5.2272 - val_loss: 5.7444\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 5.2270 - val_loss: 5.7442\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 5.2268 - val_loss: 5.7441\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 5.2267 - val_loss: 5.7439\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 5.2265 - val_loss: 5.7438\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 5.2263 - val_loss: 5.7436\n",
      "Epoch 1/8\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "Epoch 2/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "Epoch 3/8\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "Epoch 4/8\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "Epoch 5/8\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "Epoch 6/8\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "Epoch 7/8\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "Epoch 8/8\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 5.2276 - val_loss: 5.7449\n",
      "tf.Tensor(0.00021045591359050817, shape=(), dtype=float64)\n",
      "tf.Tensor(1.8381244608402737e-07, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matrix_decompositions_tf as fctr\n",
    "import transforms as transf\n",
    "import math\n",
    "import post_process_grad as ppg\n",
    "\n",
    "fltrSz = (3,3)\n",
    "fftSz = (8,8)\n",
    "noc = 8\n",
    "nof = 3\n",
    "rho = 1.0\n",
    "dtype = tf.complex128\n",
    "nots = 100\n",
    "novs = 100\n",
    "\n",
    "AInv = fctr.dictionary_object2D_full(fltrSz=fltrSz,fftSz=fftSz,noc = noc,nof = nof,rho = rho,dtype=dtype)\n",
    "\n",
    "wt = tf.complex(tf.random.normal((nots,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((nots,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wt = wt + tf.math.conj(tf.reverse(wt,axis=(2,3)))\n",
    "wt = wt[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "wv = tf.complex(tf.random.normal((novs,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((novs,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "wv = wv + tf.math.conj(tf.reverse(wv,axis=(2,3)))\n",
    "wv = wv[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "\n",
    "B = transf.fft2d_inner(fftSz)(tf.random.normal((1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype))\n",
    "\n",
    "\n",
    "xt = tf.linalg.matmul(B,wt,adjoint_a = True)\n",
    "xv = tf.linalg.matmul(B,wv,adjoint_a = True)\n",
    "idmat = tf.eye(nof,dtype=dtype,batch_shape = (int(tf.rank(B)) - 2)*(1,))\n",
    "yt = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xt)\n",
    "yv = tf.linalg.matmul(tf.linalg.inv(rho*idmat + tf.linalg.matmul(B,B,adjoint_a=True)),xv)\n",
    "\n",
    "w = tf.keras.layers.Input(shape=(fftSz[0],math.floor(fftSz[1]/2.) + 1,) + (noc,1,),dtype=dtype)\n",
    "x = tf.linalg.matmul(B,w,adjoint_a = True)\n",
    "y1 = AInv.qinv(x)\n",
    "y2 = fctr.QInv_auto(AInv.dhmul,rho)(x)\n",
    "\n",
    "model1 = ppg.Model_record_grad(w,y2)\n",
    "model1.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model1.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "model2 = ppg.Model_passenger(model1.gradient_record,w,y1)\n",
    "model2.compile(loss = tf.keras.losses.MSE,run_eagerly=True,optimizer=tf.keras.optimizers.SGD())\n",
    "model2.fit(x=wt,y=yt,batch_size=10,epochs=8,shuffle=False,validation_data = (wv,yv))\n",
    "maxError = 0.\n",
    "maxGrad = 0.\n",
    "for grad1,grad2 in zip(model1.gradient_record,model2.gradient_record):\n",
    "    for gradval1,gradval2 in zip(grad1,grad2):\n",
    "        maxError = max(maxError,tf.math.reduce_max(tf.math.abs((gradval1 - gradval2)*tf.math.conj(gradval1 - gradval2))))\n",
    "        maxGrad = max(maxGrad,tf.math.reduce_max(tf.math.abs(gradval1*tf.math.conj(gradval1))),tf.math.reduce_max(tf.math.abs(gradval2*tf.math.conj(gradval2))))\n",
    "\n",
    "print(maxGrad)\n",
    "print(maxError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
