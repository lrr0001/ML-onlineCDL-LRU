{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to add post-processing after the gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def custom_loss(y_actual, y_pred):\n",
    "    pred_rank = tf.rank(y_pred)\n",
    "    if pred_rank > 1:\n",
    "        return tf.math.reduce_sum(1/((y_actual - y_pred - 1)**2 + 0.6) - 1/((y_actual - y_pred + 1)**2 + 0.6),axis=tf.range(1,tf.rank(y_pred)))\n",
    "    else:\n",
    "        return 1/((y_actual - y_pred - 1)**2 + 0.6) - 1/((y_actual - y_pred + 1)**2 + 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom loss function is minimized when the model predicts an output 1 larger than the actual (ground truth) output. The loss function is maximized when the model predicts 1 less than actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f9a4fc8ed68>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFdCAYAAAAqi+WzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a50775cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_actual = np.arange(-2,2,0.05)\n",
    "y_predicted = np.zeros(y_actual.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "axs = fig.add_axes([0,0,1,1])\n",
    "#loss = lambda act,pred: np.array([custom_loss(act[ii],pred[ii]) for ii in range(act.shape[0])])\n",
    "\n",
    "axs.plot(y_predicted - y_actual,custom_loss(y_actual,y_predicted))\n",
    "axs.set_title('Custom Loss Function')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.set_xlabel('Predicted Minus Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, there is no post gradient computations. The network correctly finds that by adding 1 to the input, the loss is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "100/100 [==============================] - 0s 2ms/step - loss: -0.0909 - val_loss: -0.1713\n",
      "Epoch 2/16\n",
      "100/100 [==============================] - 0s 810us/step - loss: -0.2503 - val_loss: -0.3316\n",
      "Epoch 3/16\n",
      "100/100 [==============================] - 0s 939us/step - loss: -0.4130 - val_loss: -0.4971\n",
      "Epoch 4/16\n",
      "100/100 [==============================] - 0s 877us/step - loss: -0.5815 - val_loss: -0.6684\n",
      "Epoch 5/16\n",
      "100/100 [==============================] - 0s 804us/step - loss: -0.7549 - val_loss: -0.8434\n",
      "Epoch 6/16\n",
      "100/100 [==============================] - 0s 825us/step - loss: -0.9295 - val_loss: -1.0162\n",
      "Epoch 7/16\n",
      "100/100 [==============================] - 0s 825us/step - loss: -1.0971 - val_loss: -1.1765\n",
      "Epoch 8/16\n",
      "100/100 [==============================] - 0s 873us/step - loss: -1.2456 - val_loss: -1.3105\n",
      "Epoch 9/16\n",
      "100/100 [==============================] - 0s 913us/step - loss: -1.3606 - val_loss: -1.4039\n",
      "Epoch 10/16\n",
      "100/100 [==============================] - 0s 958us/step - loss: -1.4299 - val_loss: -1.4478\n",
      "Epoch 11/16\n",
      "100/100 [==============================] - 0s 856us/step - loss: -1.4516 - val_loss: -1.4524\n",
      "Epoch 12/16\n",
      "100/100 [==============================] - 0s 958us/step - loss: -1.4524 - val_loss: -1.4524\n",
      "Epoch 13/16\n",
      "100/100 [==============================] - 0s 928us/step - loss: -1.4524 - val_loss: -1.4524\n",
      "Epoch 14/16\n",
      "100/100 [==============================] - 0s 830us/step - loss: -1.4524 - val_loss: -1.4524\n",
      "Epoch 15/16\n",
      "100/100 [==============================] - 0s 874us/step - loss: -1.4524 - val_loss: -1.4524\n",
      "Epoch 16/16\n",
      "100/100 [==============================] - 0s 828us/step - loss: -1.4524 - val_loss: -1.4524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'add_var/Variable:0' shape=(1,) dtype=float32, numpy=array([1.0322956], dtype=float32)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class AddVar(tf.keras.layers.Layer):\n",
    "    def __init__(*args,**kwargs):\n",
    "        tf.keras.layers.Layer.__init__(*args,**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        self.bias = tf.Variable(initial_value=tf.zeros(input_shape[-1]),trainable=True,dtype=self.dtype)\n",
    "    def call(self,inputs):\n",
    "        return inputs + self.bias\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "outputs = AddVar()(inputs)\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "model.compile(loss = custom_loss)\n",
    "x = np.random.randn(1000,1)\n",
    "y = x\n",
    "xval = np.random.randn(100,1)\n",
    "yval = xval\n",
    "model.fit(x=x,y=y,batch_size=10,epochs=16,validation_data = (xval,yval))\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this example demonstrates adding a post-gradient process.  The post-gradient process takes the update and reverses its sign, effectively changing the algorithm to gradient ascent. As a result, Tensorflow ends up maximizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.1713\n",
      "Epoch 2/16\n",
      "100/100 [==============================] - 0s 960us/step - loss: 0.2503 - val_loss: 0.3316\n",
      "Epoch 3/16\n",
      "100/100 [==============================] - 0s 854us/step - loss: 0.4130 - val_loss: 0.4971\n",
      "Epoch 4/16\n",
      "100/100 [==============================] - 0s 937us/step - loss: 0.5815 - val_loss: 0.6684\n",
      "Epoch 5/16\n",
      "100/100 [==============================] - 0s 758us/step - loss: 0.7549 - val_loss: 0.8434\n",
      "Epoch 6/16\n",
      "100/100 [==============================] - 0s 873us/step - loss: 0.9295 - val_loss: 1.0162\n",
      "Epoch 7/16\n",
      "100/100 [==============================] - 0s 872us/step - loss: 1.0971 - val_loss: 1.1765\n",
      "Epoch 8/16\n",
      "100/100 [==============================] - 0s 865us/step - loss: 1.2456 - val_loss: 1.3105\n",
      "Epoch 9/16\n",
      "100/100 [==============================] - 0s 802us/step - loss: 1.3606 - val_loss: 1.4039\n",
      "Epoch 10/16\n",
      "100/100 [==============================] - 0s 802us/step - loss: 1.4299 - val_loss: 1.4478\n",
      "Epoch 11/16\n",
      "100/100 [==============================] - 0s 859us/step - loss: 1.4516 - val_loss: 1.4524\n",
      "Epoch 12/16\n",
      "100/100 [==============================] - 0s 872us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 13/16\n",
      "100/100 [==============================] - 0s 858us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 14/16\n",
      "100/100 [==============================] - 0s 851us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 15/16\n",
      "100/100 [==============================] - 0s 772us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 16/16\n",
      "100/100 [==============================] - 0s 823us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "INFO:tensorflow:Assets written to: This_is_a_model_i_saved.pb/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PostProcess:\n",
    "    update = {}\n",
    "    def add_update(varName,update_fun):\n",
    "        assert varName not in PostProcess.update, \"Update function already exists for %r; may be duplicate\" % varName\n",
    "        PostProcess.update[varName] = update_fun\n",
    "def mysum(x):\n",
    "    a,b = x\n",
    "    return a + b\n",
    "\n",
    "class AddVar(tf.keras.layers.Layer,PostProcess):\n",
    "    def __init__(*args,**kwargs):\n",
    "        tf.keras.layers.Layer.__init__(*args,**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        self.bias = tf.Variable(initial_value=tf.zeros(input_shape[-1]),trainable=False,dtype=self.dtype)\n",
    "        self.bias2 = tf.Variable(initial_value=tf.identity(self.bias),trainable=True,dtype=self.dtype)\n",
    "        PostProcess.add_update(self.bias2.name, self.post_update)\n",
    "    def call(self,inputs):\n",
    "        x = (inputs,self.bias2)\n",
    "        return mysum(x)\n",
    "    def post_update(self):\n",
    "\n",
    "        theupdate = self.bias2 - self.bias\n",
    "        # If \"=\" is used for assignment, differentiation will fail (in eager execution) or throw an error.\n",
    "        # The assign operator is required.        \n",
    "        newBias2 = self.bias2.assign(self.bias - theupdate/2.)\n",
    "        newBias2 = self.bias2.assign(newBias2 - theupdate/2.)\n",
    "        return [self.bias.assign(newBias2)]\n",
    "\n",
    "class CustomTrainModel(tf.keras.Model):\n",
    "    def train_step(self,data):\n",
    "        myoutputs = tf.keras.Model.train_step(self,data)\n",
    "        for tv in self.trainable_variables:\n",
    "            if tv.name in PostProcess.update:\n",
    "                PostProcess.update[tv.name]()\n",
    "        return myoutputs\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "outputs = AddVar()(inputs)\n",
    "model = CustomTrainModel(inputs,outputs)\n",
    "model.compile(loss = custom_loss,run_eagerly=False)\n",
    "x = np.random.randn(1000,1)\n",
    "y = x\n",
    "xval = np.random.randn(100,1)\n",
    "yval = xval\n",
    "model.fit(x=x,y=y,batch_size=10,epochs=16,validation_data = (xval,yval))\n",
    "model.trainable_variables\n",
    "model.save('This_is_a_model_i_saved.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.1713\n",
      "Epoch 2/16\n",
      "100/100 [==============================] - 0s 946us/step - loss: 0.2503 - val_loss: 0.3316\n",
      "Epoch 3/16\n",
      "100/100 [==============================] - 0s 898us/step - loss: 0.4130 - val_loss: 0.4971\n",
      "Epoch 4/16\n",
      "100/100 [==============================] - 0s 843us/step - loss: 0.5815 - val_loss: 0.6684\n",
      "Epoch 5/16\n",
      "100/100 [==============================] - 0s 839us/step - loss: 0.7549 - val_loss: 0.8434\n",
      "Epoch 6/16\n",
      "100/100 [==============================] - 0s 849us/step - loss: 0.9295 - val_loss: 1.0162\n",
      "Epoch 7/16\n",
      "100/100 [==============================] - 0s 973us/step - loss: 1.0971 - val_loss: 1.1765\n",
      "Epoch 8/16\n",
      "100/100 [==============================] - 0s 864us/step - loss: 1.2456 - val_loss: 1.3105\n",
      "Epoch 9/16\n",
      "100/100 [==============================] - 0s 859us/step - loss: 1.3606 - val_loss: 1.4039\n",
      "Epoch 10/16\n",
      "100/100 [==============================] - 0s 920us/step - loss: 1.4299 - val_loss: 1.4478\n",
      "Epoch 11/16\n",
      "100/100 [==============================] - 0s 846us/step - loss: 1.4516 - val_loss: 1.4524\n",
      "Epoch 12/16\n",
      "100/100 [==============================] - 0s 878us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 13/16\n",
      "100/100 [==============================] - 0s 842us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 14/16\n",
      "100/100 [==============================] - 0s 880us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 15/16\n",
      "100/100 [==============================] - 0s 899us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 16/16\n",
      "100/100 [==============================] - 0s 898us/step - loss: 1.4524 - val_loss: 1.4524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'add_var/Variable:0' shape=(1,) dtype=float32, numpy=array([-1.0332956], dtype=float32)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PostProcess:\n",
    "    update = {}\n",
    "    def add_update(varName,update_fun):\n",
    "        assert varName not in PostProcess.update, \"Update function already exists for %r; may be duplicate\" % varName\n",
    "        PostProcess.update[varName] = update_fun\n",
    "def mysum(x):\n",
    "    return x[0] + x[1]\n",
    "\n",
    "class AddVar(tf.keras.layers.Layer,PostProcess):\n",
    "    def __init__(*args,**kwargs):\n",
    "        tf.keras.layers.Layer.__init__(*args,**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        self.bias = tf.Variable(initial_value=tf.zeros(input_shape[-1]),trainable=False,dtype=self.dtype)\n",
    "        self.bias2 = tf.Variable(initial_value=tf.identity(self.bias),trainable=True,dtype=self.dtype)\n",
    "        PostProcess.add_update(self.bias2.name, self.post_update)\n",
    "    def call(self,inputs):\n",
    "        x = []\n",
    "        x.append(inputs)\n",
    "        x.append(self.bias2)\n",
    "        return mysum(x)\n",
    "    def post_update(self):\n",
    "\n",
    "        theupdate = self.bias2 - self.bias\n",
    "        # If \"=\" is used for assignment, differentiation will fail (in eager execution) or throw an error.\n",
    "        # The assign operator is required.        \n",
    "        newBias2 = self.bias2.assign(self.bias - theupdate)\n",
    "        return [self.bias.assign(newBias2)]\n",
    "\n",
    "class CustomTrainModel(tf.keras.Model):\n",
    "    def train_step(self,data):\n",
    "        myoutputs = tf.keras.Model.train_step(self,data)\n",
    "        for tv in self.trainable_variables:\n",
    "            if tv.name in PostProcess.update:\n",
    "                PostProcess.update[tv.name]()\n",
    "        return myoutputs\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "outputs = AddVar()(inputs)\n",
    "model = CustomTrainModel(inputs,outputs)\n",
    "model.compile(loss = custom_loss,run_eagerly=False)\n",
    "x = np.random.randn(1000,1)\n",
    "y = x\n",
    "xval = np.random.randn(100,1)\n",
    "yval = xval\n",
    "model.fit(x=x,y=y,batch_size=10,epochs=16,validation_data = (xval,yval))\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.1713\n",
      "Epoch 2/16\n",
      "100/100 [==============================] - 0s 971us/step - loss: 0.2503 - val_loss: 0.3316\n",
      "Epoch 3/16\n",
      "100/100 [==============================] - 0s 946us/step - loss: 0.4130 - val_loss: 0.4971\n",
      "Epoch 4/16\n",
      "100/100 [==============================] - 0s 875us/step - loss: 0.5815 - val_loss: 0.6684\n",
      "Epoch 5/16\n",
      "100/100 [==============================] - 0s 950us/step - loss: 0.7549 - val_loss: 0.8434\n",
      "Epoch 6/16\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.9295 - val_loss: 1.0162\n",
      "Epoch 7/16\n",
      "100/100 [==============================] - 0s 898us/step - loss: 1.0971 - val_loss: 1.1765\n",
      "Epoch 8/16\n",
      "100/100 [==============================] - 0s 970us/step - loss: 1.2456 - val_loss: 1.3105\n",
      "Epoch 9/16\n",
      "100/100 [==============================] - 0s 894us/step - loss: 1.3606 - val_loss: 1.4039\n",
      "Epoch 10/16\n",
      "100/100 [==============================] - 0s 896us/step - loss: 1.4299 - val_loss: 1.4478\n",
      "Epoch 11/16\n",
      "100/100 [==============================] - 0s 968us/step - loss: 1.4516 - val_loss: 1.4524\n",
      "Epoch 12/16\n",
      "100/100 [==============================] - 0s 923us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 13/16\n",
      "100/100 [==============================] - 0s 974us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 14/16\n",
      "100/100 [==============================] - 0s 906us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 15/16\n",
      "100/100 [==============================] - 0s 866us/step - loss: 1.4524 - val_loss: 1.4524\n",
      "Epoch 16/16\n",
      "100/100 [==============================] - 0s 893us/step - loss: 1.4524 - val_loss: 1.4524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'add_var_3/Variable:0' shape=(1,) dtype=float32, numpy=array([-1.0332959], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class PostProcess:\n",
    "    update = {}\n",
    "    def add_update(varName,update_fun):\n",
    "        assert varName not in PostProcess.update, \"Update function already exists for %r; may be duplicate\" % varName\n",
    "        PostProcess.update[varName] = update_fun\n",
    "def mysum(x):\n",
    "    return x[0] + x[1]\n",
    "\n",
    "class AddVar(tf.keras.layers.Layer,PostProcess):\n",
    "    def __init__(*args,**kwargs):\n",
    "        tf.keras.layers.Layer.__init__(*args,**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        self.bias = tf.Variable(initial_value=tf.zeros(input_shape[0][-1]),trainable=False,dtype=self.dtype)\n",
    "        self.bias2 = tf.Variable(initial_value=tf.identity(self.bias),trainable=True,dtype=self.dtype)\n",
    "        PostProcess.add_update(self.bias2.name, self.post_update)\n",
    "    def call(self,inputs):\n",
    "        xA,xB = inputs\n",
    "        return xA + xB + self.bias2\n",
    "    def post_update(self):\n",
    "\n",
    "        theupdate = self.bias2 - self.bias\n",
    "        # If \"=\" is used for assignment, differentiation will fail (in eager execution) or throw an error.\n",
    "        # The assign operator is required.        \n",
    "        newBias2 = self.bias2.assign(self.bias - theupdate)\n",
    "        return [self.bias.assign(newBias2)]\n",
    "\n",
    "class CustomTrainModel(tf.keras.Model):\n",
    "    def train_step(self,data):\n",
    "        myoutputs = tf.keras.Model.train_step(self,data)\n",
    "        for tv in self.trainable_variables:\n",
    "            if tv.name in PostProcess.update:\n",
    "                PostProcess.update[tv.name]()\n",
    "        return myoutputs\n",
    "\n",
    "inputA = tf.keras.layers.Input(shape=(1,))\n",
    "inputB = tf.keras.layers.Input(shape=(1,))\n",
    "inputs = (inputA,inputB)\n",
    "outputs = AddVar()(inputs)\n",
    "model = CustomTrainModel(inputs,outputs)\n",
    "model.compile(loss = custom_loss,run_eagerly=False)\n",
    "xA = np.random.randn(1000,1)\n",
    "xB = np.random.randn(1000,1)\n",
    "y = xA + xB\n",
    "xAval = np.random.randn(100,1)\n",
    "xBval = np.random.randn(100,1)\n",
    "yval = xAval + xBval\n",
    "model.fit(x=(xA,xB),y=y,batch_size=10,epochs=16,validation_data = ((xAval,xBval),yval))\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
