{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard form appears to be working. A dictionary object constructed without the low-rank approximations yields a nearly identical state after 1 iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(20821.854589822724, shape=(), dtype=float64)\n",
      "tf.Tensor(20821.854589822724, shape=(), dtype=float64)\n",
      "tf.Tensor(2.0605739337042905e-13, shape=(), dtype=float64)\n",
      "tf.Tensor(4.463096558993129e-14, shape=(), dtype=float64)\n",
      "tf.Tensor(1.075541985139004e-14, shape=(), dtype=float64)\n",
      "tf.Tensor(1.075541985139004e-14, shape=(), dtype=float64)\n",
      "tf.Tensor(3.552713678800501e-15, shape=(), dtype=float64)\n",
      "tf.Tensor(1.2212453270876722e-15, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matrix_decompositions_tf as fctr\n",
    "import transforms as transf\n",
    "import post_process_grad as ppg\n",
    "\n",
    "fltrSz = (3,3)\n",
    "fftSz = (8,8)\n",
    "noc = 5\n",
    "nof = 4\n",
    "rho = 1.0\n",
    "nots = 100\n",
    "lraParam = {'n_components': 5}\n",
    "dtype = tf.complex128\n",
    "\n",
    "D = tf.random.normal(shape=(1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype)\n",
    "dictObj = fctr.dictionary_object2D_init(fftSz=fftSz,D=D,rho=rho,name='dictObj1',lraParam=lraParam)\n",
    "dictObj2 = fctr.dictionary_object2D_init_full(fftSz=fftSz,D=D,rho=rho,name='dictObj2',lraParam=lraParam)\n",
    "\n",
    "\n",
    "w = tf.complex(tf.random.normal((nots,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((nots,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "w = w + tf.math.conj(tf.reverse(w,axis=(2,3)))\n",
    "w = w[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "\n",
    "B = transf.fft2d_inner(fftSz)(tf.random.normal((1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype))\n",
    "\n",
    "x = tf.linalg.matmul(B,w,adjoint_a=True)\n",
    "xi = tf.keras.layers.Input(x.shape[1:],dtype=dtype)\n",
    "y = rho*xi + tf.linalg.matmul(B,tf.linalg.matmul(B,xi),adjoint_a=True)\n",
    "z = dictObj.qinv(y)\n",
    "z2 = dictObj2.qinv(y)\n",
    "modelA = tf.keras.Model(xi,z)\n",
    "modelB = tf.keras.Model(xi,z2)\n",
    "xt = x[slice(0,10,1),slice(None),slice(None),slice(None),slice(None)]\n",
    "\n",
    "optimizerA=tf.keras.optimizers.SGD(0.001)\n",
    "optimizerB=tf.keras.optimizers.SGD(0.001)\n",
    "with tf.GradientTape() as tape:\n",
    "    zt = modelA(xt)\n",
    "    loss = tf.keras.losses.MSE(xt,zt)\n",
    "\n",
    "grad = tape.gradient(target=loss,sources=modelA.trainable_variables)\n",
    "optimizerA.apply_gradients(zip(grad,modelA.trainable_variables))\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    zt2 = modelB(xt)\n",
    "    loss2 = tf.keras.losses.MSE(xt,zt2)\n",
    "\n",
    "grad2 = tape2.gradient(target=loss2,sources=modelB.trainable_variables)\n",
    "optimizerB.apply_gradients(zip(grad2,modelB.trainable_variables))\n",
    "\n",
    "print(tf.math.reduce_max(tf.abs(dictObj2.dhmul.Df - dictObj2.dhmul.Dfprev)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.dhmul.Df - dictObj.dhmul.Dfprev)))\n",
    "\n",
    "a = dictObj._dict_update()\n",
    "b = dictObj2._dict_update()\n",
    "currMatL = tf.linalg.matmul(dictObj.qinv.L,dictObj.qinv.L,adjoint_b=True)\n",
    "currMatL2 = tf.linalg.matmul(dictObj2.qinv.L,dictObj2.qinv.L,adjoint_b=True)\n",
    "\n",
    "print(tf.math.reduce_max(tf.abs(currMatL - currMatL2)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.qinv.L - dictObj2.qinv.L)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.dhmul.Df - dictObj2.dhmul.Df)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.dhmul.Dfprev - dictObj2.dhmul.Dfprev)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.D - dictObj2.D)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.R - dictObj2.R)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woodbury Form is broken for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(27287.446691035606, shape=(), dtype=float64)\n",
      "tf.Tensor(27287.446691035606, shape=(), dtype=float64)\n",
      "tf.Tensor(15.465430409556262, shape=(), dtype=float64)\n",
      "tf.Tensor(391.98520489116015, shape=(), dtype=float64)\n",
      "tf.Tensor(14.17851868283215, shape=(), dtype=float64)\n",
      "tf.Tensor(2.6852712547870858e-15, shape=(), dtype=float64)\n",
      "tf.Tensor(2.6852712547870858e-15, shape=(), dtype=float64)\n",
      "tf.Tensor(9.992007221626409e-16, shape=(), dtype=float64)\n",
      "tf.Tensor(3.3306690738754696e-16, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matrix_decompositions_tf as fctr\n",
    "import transforms as transf\n",
    "import post_process_grad as ppg\n",
    "\n",
    "fltrSz = (3,3)\n",
    "fftSz = (8,8)\n",
    "noc = 2\n",
    "nof = 8\n",
    "rho = 1.0\n",
    "nots = 100\n",
    "lraParam = {'n_components': 2}\n",
    "dtype = tf.complex128\n",
    "\n",
    "D = tf.random.normal(shape=(1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype)\n",
    "dictObj = fctr.dictionary_object2D_init(fftSz=fftSz,D=D,rho=rho,name='dictObj1',lraParam=lraParam)\n",
    "dictObj2 = fctr.dictionary_object2D_init_full(fftSz=fftSz,D=D,rho=rho,name='dictObj2',lraParam=lraParam)\n",
    "\n",
    "\n",
    "w = tf.complex(tf.random.normal((nots,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((nots,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "w = w + tf.math.conj(tf.reverse(w,axis=(2,3)))\n",
    "w = w[slice(None),slice(None),slice(0,5,1),slice(None),slice(None)]\n",
    "\n",
    "B = transf.fft2d_inner(fftSz)(tf.random.normal((1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype))\n",
    "\n",
    "x = tf.linalg.matmul(B,w,adjoint_a=True)\n",
    "xi = tf.keras.layers.Input(x.shape[1:],dtype=dtype)\n",
    "y = rho*xi + tf.linalg.matmul(B,tf.linalg.matmul(B,xi),adjoint_a=True)\n",
    "z = dictObj.qinv(y)\n",
    "z2 = dictObj2.qinv(y)\n",
    "modelA = tf.keras.Model(xi,z)\n",
    "modelB = tf.keras.Model(xi,z2)\n",
    "xt = x[slice(0,10,1),slice(None),slice(None),slice(None),slice(None)]\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    zt = modelA(xt)\n",
    "    loss = tf.keras.losses.MSE(xt,zt)\n",
    "\n",
    "grad = tape.gradient(target=loss,sources=modelA.trainable_variables)\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    zt2 = modelB(xt)\n",
    "    loss2 = tf.keras.losses.MSE(xt,zt2)\n",
    "\n",
    "grad2 = tape2.gradient(target=loss2,sources=modelB.trainable_variables)\n",
    "\n",
    "optimizerA=tf.keras.optimizers.SGD(0.001)\n",
    "optimizerB=tf.keras.optimizers.SGD(0.001)\n",
    "\n",
    "optimizerA.apply_gradients(zip(grad,modelA.trainable_variables))\n",
    "\n",
    "optimizerB.apply_gradients(zip(grad2,modelB.trainable_variables))\n",
    "\n",
    "print(tf.math.reduce_max(tf.abs(dictObj2.dhmul.Df - dictObj2.dhmul.Dfprev)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.dhmul.Df - dictObj.dhmul.Dfprev)))\n",
    "Lprev = tf.Variable(dictObj.qinv.L)\n",
    "a=dictObj._dict_update()\n",
    "b=dictObj2._dict_update()\n",
    "currMatL = tf.linalg.matmul(dictObj.qinv.L,dictObj.qinv.L,adjoint_b=True)\n",
    "currMatL2 = tf.linalg.matmul(dictObj2.qinv.L,dictObj2.qinv.L,adjoint_b=True)\n",
    "\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.qinv.L - Lprev)))\n",
    "print(tf.math.reduce_max(tf.abs(currMatL - currMatL2)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.qinv.L - dictObj2.qinv.L)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.dhmul.Df - dictObj2.dhmul.Df)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.dhmul.Dfprev - dictObj2.dhmul.Dfprev)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.D - dictObj2.D)))\n",
    "print(tf.math.reduce_max(tf.abs(dictObj.R - dictObj2.R)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.9146142661633756e-14, shape=(), dtype=float64)\n",
      "tf.Tensor(3.552713678800501e-15, shape=(), dtype=float64)\n",
      "tf.Tensor(1.5298303428545825, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matrix_decompositions_tf as fctr\n",
    "import transforms as transf\n",
    "import post_process_grad as ppg\n",
    "import util\n",
    "\n",
    "fltrSz = (2,2)\n",
    "fftSz = (3,3)\n",
    "noc = 2\n",
    "nof = 3\n",
    "rho = 1.0\n",
    "nots = 100\n",
    "lraParam = {'n_components': 2}\n",
    "dtype = tf.complex128\n",
    "\n",
    "D = tf.random.normal(shape=(1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype)\n",
    "dictObj = fctr.dictionary_object2D_init(fftSz=fftSz,D=D,rho=rho,name='dictObj1',lraParam=lraParam)\n",
    "dictObj2 = fctr.dictionary_object2D_init_full(fftSz=fftSz,D=D,rho=rho,name='dictObj2',lraParam=lraParam)\n",
    "\n",
    "\n",
    "w = tf.complex(tf.random.normal((nots,) + fftSz + (noc,1),dtype=dtype.real_dtype),tf.random.normal((nots,) + fftSz + (noc,1),dtype = dtype.real_dtype))\n",
    "w = w + tf.math.conj(tf.reverse(w,axis=(2,3)))\n",
    "w = w[slice(None),slice(None),slice(0,2,1),slice(None),slice(None)]\n",
    "\n",
    "B = transf.fft2d_inner(fftSz)(tf.random.normal((1,) + fltrSz + (noc,nof,),dtype=dtype.real_dtype))\n",
    "\n",
    "x = tf.linalg.matmul(B,w,adjoint_a=True)\n",
    "xi = tf.keras.layers.Input(x.shape[1:],dtype=dtype)\n",
    "y = rho*xi + tf.linalg.matmul(B,tf.linalg.matmul(B,xi),adjoint_a=True)\n",
    "z = dictObj.qinv(y)\n",
    "z2 = dictObj2.qinv(y)\n",
    "modelA = tf.keras.Model(xi,z)\n",
    "modelB = tf.keras.Model(xi,z2)\n",
    "xt = x[slice(0,10,1),slice(None),slice(None),slice(None),slice(None)]\n",
    "\n",
    "optimizerA=tf.keras.optimizers.SGD(0.001)\n",
    "optimizerB=tf.keras.optimizers.SGD(0.001)\n",
    "with tf.GradientTape() as tape:\n",
    "    zt = modelA(xt)\n",
    "    loss = tf.keras.losses.MSE(xt,zt)\n",
    "\n",
    "grad = tape.gradient(target=loss,sources=modelA.trainable_variables)\n",
    "optimizerA.apply_gradients(zip(grad,modelA.trainable_variables))\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    zt2 = modelB(xt)\n",
    "    loss2 = tf.keras.losses.MSE(xt,zt2)\n",
    "\n",
    "grad2 = tape2.gradient(target=loss2,sources=modelB.trainable_variables)\n",
    "optimizerB.apply_gradients(zip(grad2,modelB.trainable_variables))\n",
    "\n",
    "L = tf.Variable(dictObj.qinv.L)\n",
    "currMatL = tf.linalg.matmul(L,L,adjoint_b=True)\n",
    "\n",
    "Dnew = dictObj.get_constrained_D(dictObj.dhmul.Df)\n",
    "\n",
    "# compute low rank approximation of the update\n",
    "theUpdate = Dnew - dictObj.D\n",
    "U,V,approx = fctr.stack_svd(theUpdate,5,**dictObj.lraParam)\n",
    "\n",
    "# Compute DFT (The conjugate is necessary because F{A} = F{U}F{V}^T\n",
    "Uf = util.complexNum(U)\n",
    "Vf = tf.math.conj(transf.fft2d_inner(dictObj.fftSz)(V))\n",
    "\n",
    "# Update Decomposition and Frequency-Domain Dictionary\n",
    "Lr1 = tf.Variable(dictObj._rank1_updates(Uf,Vf,L))\n",
    "VhV = tf.math.reduce_sum(Vf*tf.math.conj(Vf),axis=3,keepdims=True)\n",
    "rank1update = tf.linalg.matmul(VhV*Uf,Uf,adjoint_b=True)\n",
    "\n",
    "currMatLr1u = tf.Variable(tf.linalg.matmul(dictObj.qinv.L,dictObj.qinv.L,adjoint_b=True))\n",
    "\n",
    "print(tf.math.reduce_sum(tf.math.abs(currMatLr1u - currMatL - rank1update)))\n",
    "\n",
    "\n",
    "Dv = tf.linalg.matmul(dictObj.dhmul.Dfprev,Vf)\n",
    "rank2update = tf.linalg.matmul(Dv,Uf,adjoint_b=True) + tf.linalg.matmul(Uf,Dv,adjoint_b=True)\n",
    "\n",
    "eigvals,eigvecs,asVec = dictObj._get_eigen_decomp(Uf,Vf,dictObj.dhmul.Dfprev)\n",
    "eigsum = 0.\n",
    "for vals,vecs in zip(eigvals,eigvecs):\n",
    "    for val,vec in zip(tf.unstack(vals,axis=0),tf.unstack(vecs,axis=0)):\n",
    "        eigsum = eigsum + util.addDim(util.addDim(val))*tf.linalg.matmul(util.addDim(vec),util.addDim(vec),adjoint_b = True)\n",
    "\n",
    "print(tf.reduce_max(tf.math.abs(rank2update - eigsum)))\n",
    "\n",
    "Lr2 = dictObj._eig_chol_update(eigvals,eigvecs,dictObj.qinv.L)\n",
    "currMatLr2u = tf.Variable(tf.linalg.matmul(dictObj.qinv.L,dictObj.qinv.L,adjoint_b = True))\n",
    "\n",
    "\n",
    "print(tf.reduce_max(tf.math.abs(currMatLr2u - currMatLr1u - eigsum)))\n",
    "\n",
    "#print(currMatLr1u)\n",
    "\n",
    "#Lr2,asVec = dictObj._rank2_updates(Uf,Vf,dictObj.dhmul.Dfprev,Lr1)\n",
    "#Lr2 = tf.Variable(Lr2)\n",
    "\n",
    "#currMatLr2u = tf.linalg.matmul(Lr2,Lr2,adjoint_b=True)\n",
    "\n",
    "#print(rank2update)\n",
    "#print(currMatLr2u - currMatLr1u - rank2update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'currMatLr1u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a3737937b2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrMatLr1u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'currMatLr1u' is not defined"
     ]
    }
   ],
   "source": [
    "print(currMatLr1u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=complex128, numpy=\n",
       "array([[ 70.44826957+0.j, -32.84980158+0.j, -25.16938287+0.j],\n",
       "       [-32.84980158+0.j,  26.4083223 +0.j,   4.06463908+0.j],\n",
       "       [-25.16938287+0.j,   4.06463908+0.j,  29.8609006 +0.j]])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currMatLr1u[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 5, 1, 3), dtype=complex128, numpy=\n",
       "array([[[[[1.24372066+0.j, 0.65238577+0.j, 0.12519912+0.j]],\n",
       "\n",
       "         [[1.29827414+0.j, 0.83938382+0.j, 0.70425554+0.j]],\n",
       "\n",
       "         [[1.24746564+0.j, 1.03418915+0.j, 1.46738332+0.j]],\n",
       "\n",
       "         [[0.93854587+0.j, 0.86603954+0.j, 1.33271449+0.j]],\n",
       "\n",
       "         [[0.73498807+0.j, 0.69008266+0.j, 1.01397429+0.j]]],\n",
       "\n",
       "\n",
       "        [[[1.08630945+0.j, 0.77098751+0.j, 0.58690447+0.j]],\n",
       "\n",
       "         [[1.25054204+0.j, 0.9121986 +0.j, 0.43655462+0.j]],\n",
       "\n",
       "         [[1.01606036+0.j, 0.98475371+0.j, 0.71677172+0.j]],\n",
       "\n",
       "         [[0.70281923+0.j, 0.79601501+0.j, 0.76543103+0.j]],\n",
       "\n",
       "         [[0.99947963+0.j, 0.76440402+0.j, 0.79984342+0.j]]],\n",
       "\n",
       "\n",
       "        [[[0.79667395+0.j, 0.88456978+0.j, 1.18298464+0.j]],\n",
       "\n",
       "         [[1.14937826+0.j, 0.96023416+0.j, 0.80710283+0.j]],\n",
       "\n",
       "         [[1.07615086+0.j, 1.26854068+0.j, 0.60975228+0.j]],\n",
       "\n",
       "         [[0.91165808+0.j, 1.23011594+0.j, 0.61948086+0.j]],\n",
       "\n",
       "         [[1.25864392+0.j, 0.90349772+0.j, 0.69258987+0.j]]],\n",
       "\n",
       "\n",
       "        [[[0.63486749+0.j, 0.75384995+0.j, 1.04568879+0.j]],\n",
       "\n",
       "         [[0.95349195+0.j, 0.89389561+0.j, 1.04152538+0.j]],\n",
       "\n",
       "         [[0.98692803+0.j, 1.47268233+0.j, 1.07166816+0.j]],\n",
       "\n",
       "         [[0.88661828+0.j, 1.53200361+0.j, 1.17246253+0.j]],\n",
       "\n",
       "         [[0.98129122+0.j, 0.98555063+0.j, 1.16474544+0.j]]],\n",
       "\n",
       "\n",
       "        [[[0.60528533+0.j, 0.6281496 +0.j, 0.77401812+0.j]],\n",
       "\n",
       "         [[0.63775331+0.j, 0.91930036+0.j, 0.8746739 +0.j]],\n",
       "\n",
       "         [[0.69175634+0.j, 1.28805022+0.j, 1.13190356+0.j]],\n",
       "\n",
       "         [[0.71127859+0.j, 1.18424026+0.j, 1.40925055+0.j]],\n",
       "\n",
       "         [[0.70926577+0.j, 1.00283119+0.j, 1.53002367+0.j]]],\n",
       "\n",
       "\n",
       "        [[[0.63486749+0.j, 0.75384995+0.j, 1.04568879+0.j]],\n",
       "\n",
       "         [[0.48766899+0.j, 1.08302298+0.j, 0.96160044+0.j]],\n",
       "\n",
       "         [[0.76915226+0.j, 1.0694285 +0.j, 0.89252077+0.j]],\n",
       "\n",
       "         [[1.04445979+0.j, 0.77258922+0.j, 0.99903481+0.j]],\n",
       "\n",
       "         [[0.98129122+0.j, 0.98555063+0.j, 1.16474544+0.j]]],\n",
       "\n",
       "\n",
       "        [[[0.79667395+0.j, 0.88456978+0.j, 1.18298464+0.j]],\n",
       "\n",
       "         [[0.73103369+0.j, 1.12189977+0.j, 1.37921139+0.j]],\n",
       "\n",
       "         [[1.28267937+0.j, 1.13442784+0.j, 1.19377873+0.j]],\n",
       "\n",
       "         [[1.62207807+0.j, 0.87878615+0.j, 0.87331043+0.j]],\n",
       "\n",
       "         [[1.25864392+0.j, 0.90349772+0.j, 0.69258987+0.j]]],\n",
       "\n",
       "\n",
       "        [[[1.08630945+0.j, 0.77098751+0.j, 0.58690447+0.j]],\n",
       "\n",
       "         [[1.12473642+0.j, 0.95170093+0.j, 1.32556324+0.j]],\n",
       "\n",
       "         [[1.52591156+0.j, 1.19834335+0.j, 1.72185724+0.j]],\n",
       "\n",
       "         [[1.54966332+0.j, 1.05857405+0.j, 1.29782797+0.j]],\n",
       "\n",
       "         [[0.99947963+0.j, 0.76440402+0.j, 0.79984342+0.j]]]]])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VhV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
